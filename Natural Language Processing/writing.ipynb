{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **retrain the model on ambiguous data points** from the given plot to improve the model's accuracy, you should follow a focused and methodical retraining approach that aims to help the model better understand the challenging, uncertain areas. Below, I describe the steps that can help you leverage the ambiguous data points effectively:\n",
    "\n",
    "### **1. Identify and Extract Ambiguous Data Points**\n",
    "- **From the Plot**: Ambiguous data points are typically characterized by **moderate confidence** (between **0.4 - 0.6**) and **high variability** (greater than **0.3**).\n",
    "- Use these boundaries to identify the ambiguous data points from your dataset.\n",
    "- Extract the examples in this region and create a separate dataset that you can use specifically for retraining.\n",
    "\n",
    "### **2. Emphasize Ambiguous Examples During Retraining**\n",
    "- **Create a Balanced Dataset**: Combine the ambiguous examples with a subset of easy-to-learn and hard-to-learn examples for retraining.\n",
    "  - You could create a training set with an **increased proportion of ambiguous examples**, ensuring that the model receives more exposure to the data that previously confused it. This will help the model learn more nuanced decision boundaries.\n",
    "  - A reasonable strategy might involve creating a dataset that has around **50% ambiguous examples**, **25% easy-to-learn**, and **25% hard-to-learn** examples.\n",
    "  \n",
    "- **Apply Data Augmentation**: Since ambiguous examples often involve nuanced relationships or variations, you can apply **data augmentation** techniques to generate additional similar examples. For instance:\n",
    "  - Use **paraphrasing** to introduce slight changes in the wording without altering the meaning.\n",
    "  - Introduce **synonym substitution** or **syntactic rephrasing** to encourage the model to be more adaptive.\n",
    "\n",
    "### **3. Use Curriculum Learning Strategy**\n",
    "- **Gradual Exposure**: Use a **curriculum learning strategy** where the model first re-trains on easy-to-learn examples, followed by ambiguous and hard-to-learn examples. This helps the model gradually improve its capability to handle more difficult examples.\n",
    "- **Weighted Loss**: Consider using a **weighted loss function** that places greater emphasis on correctly classifying ambiguous examples. This encourages the model to focus on improving predictions in areas where it previously demonstrated uncertainty.\n",
    "\n",
    "### **4. Implement Targeted Fine-Tuning**\n",
    "- **Fine-Tune with a Lower Learning Rate**: When retraining the model on ambiguous examples, use a **lower learning rate**. This ensures that the model does not \"forget\" previously learned patterns but instead **refines** its ability to classify uncertain or nuanced instances more accurately.\n",
    "- **Use Validation Metrics to Monitor Improvement**: Monitor validation accuracy not only on ambiguous examples but also on easy and hard-to-learn examples. This helps in ensuring that the model maintains or improves overall performance without overfitting to ambiguous data.\n",
    "\n",
    "### **5. Introduce Knowledge Distillation (Optional)**\n",
    "- **Knowledge Distillation**: If you have a larger, more complex model (a \"teacher model\") that performs well, use **knowledge distillation**. Train the smaller model (ELECTRA-small) to learn from the teacher’s outputs on ambiguous examples. This approach can be helpful when the smaller model struggles to make nuanced decisions that the larger model can handle.\n",
    "\n",
    "### **6. Re-Evaluate Using Contrast Sets**\n",
    "- After retraining on ambiguous examples, evaluate the model using the **contrast sets** you have previously created (e.g., synonym substitutions, syntactic rephrasings, and semantic shifts).\n",
    "- Compare the performance of the original model versus the retrained model to determine if the targeted retraining has improved **generalization** and the ability to handle nuanced linguistic variations.\n",
    "\n",
    "### **7. Iterative Retraining**\n",
    "- **Iterative Process**: Retraining should be conducted iteratively. After each iteration:\n",
    "  - Replot the variability versus confidence chart.\n",
    "  - Evaluate if there has been a reduction in the number of **ambiguous data points** (i.e., if more points have shifted to the easy-to-learn cluster).\n",
    "  - Continue retraining until you observe a noticeable increase in the model’s confidence and a reduction in variability for the previously ambiguous data points.\n",
    "\n",
    "### **Potential Benefits of Retraining on Ambiguous Data Points**\n",
    "- **Better Generalization**: The model learns to better handle cases that lie near decision boundaries, thus becoming less reliant on superficial patterns and more capable of capturing true semantic relationships.\n",
    "- **Reduced Overfitting to Artifacts**: Since ambiguous examples often involve nuanced relationships, retraining on them helps the model avoid overfitting to dataset artifacts and instead learn a deeper representation of language.\n",
    "- **Increased Robustness**: By focusing on previously challenging examples, the model is likely to be more **robust** when faced with similar out-of-distribution (OOD) or adversarial examples in the future.\n",
    "\n",
    "### **Summary of Approach**\n",
    "1. **Identify Ambiguous Examples**: Extract examples from the region of **moderate confidence** (0.4 - 0.6) and **high variability** (> 0.3).\n",
    "2. **Create Training Dataset**: Emphasize ambiguous examples in the new training set, with a balanced mix of easy and hard-to-learn examples.\n",
    "3. **Curriculum Learning and Weighted Loss**: Train gradually, starting with easier examples, and use a weighted loss to prioritize ambiguous examples.\n",
    "4. **Fine-Tune with Low Learning Rate**: Ensure that the model refines its decision-making abilities without forgetting prior knowledge.\n",
    "5. **Evaluate Using Contrast Sets**: Use contrast sets to validate the improvements in robustness.\n",
    "6. **Iterative Retraining**: Continuously assess and retrain until a significant improvement is observed.\n",
    "\n",
    "By focusing on ambiguous data points, the model can improve its overall performance, specifically in scenarios requiring nuanced understanding, which will likely translate to higher accuracy on both in-distribution and out-of-distribution datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run **Dataset Cartography** on the **SNLI dataset**, you need to train your model while carefully tracking key metrics for each data point across multiple epochs. The process allows you to classify data points into **easy-to-learn**, **hard-to-learn**, and **ambiguous examples** based on their **confidence** and **variability**. Below, I outline a step-by-step guide to running Dataset Cartography on the SNLI dataset:\n",
    "\n",
    "### **Step 1: Prepare the SNLI Dataset and Model**\n",
    "1. **Load the SNLI Dataset**:\n",
    "   - Use an NLP library like **Hugging Face's Datasets** or **torchtext** to load the SNLI dataset.\n",
    "   - You need to split the dataset into **training**, **validation**, and **test** sets.\n",
    "\n",
    "   ```python\n",
    "   from datasets import load_dataset\n",
    "\n",
    "   dataset = load_dataset('snli')\n",
    "   train_data = dataset['train']\n",
    "   validation_data = dataset['validation']\n",
    "   ```\n",
    "\n",
    "2. **Choose a Pre-trained Model**:\n",
    "   - Use a transformer model like **ELECTRA** from the **Hugging Face Transformers** library. You can choose the small version for a balance between training efficiency and performance.\n",
    "\n",
    "   ```python\n",
    "   from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
    "\n",
    "   model = ElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n",
    "   tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "   ```\n",
    "\n",
    "### **Step 2: Implement Training Loop and Track Metrics**\n",
    "1. **Tracking Confidence and Variability**:\n",
    "   - To run **Dataset Cartography**, you need to track the following metrics for **each data point** over multiple training epochs:\n",
    "     - **Confidence**: The average probability assigned by the model to the correct label.\n",
    "     - **Variability**: The standard deviation of confidence across epochs, which represents how consistently the model predicts each example.\n",
    "\n",
    "2. **Implement the Training Loop**:\n",
    "   - Train the model for a certain number of **epochs** (e.g., 5-10 epochs).\n",
    "   - For each data point in the **training set**, track the confidence assigned to the correct label during each epoch.\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "   from torch.utils.data import DataLoader\n",
    "   from transformers import AdamW\n",
    "\n",
    "   # Define optimizer and DataLoader\n",
    "   optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "   train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "   # Store metrics for each example\n",
    "   confidence_records = {i: [] for i in range(len(train_data))}  # To track confidence for each example\n",
    "\n",
    "   model.train()\n",
    "   for epoch in range(epochs):\n",
    "       for batch in train_loader:\n",
    "           inputs = tokenizer(batch['premise'], batch['hypothesis'], return_tensors='pt', padding=True, truncation=True)\n",
    "           labels = torch.tensor(batch['label'])\n",
    "           outputs = model(**inputs, labels=labels)\n",
    "\n",
    "           # Loss and backward propagation\n",
    "           loss = outputs.loss\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           optimizer.zero_grad()\n",
    "\n",
    "           # Get confidence scores (probabilities of correct labels)\n",
    "           logits = outputs.logits\n",
    "           probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "           correct_probs = probs[range(len(labels)), labels]\n",
    "\n",
    "           # Update confidence records for each example in the batch\n",
    "           for idx, prob in zip(batch['idx'], correct_probs):\n",
    "               confidence_records[idx].append(prob.item())\n",
    "   ```\n",
    "\n",
    "3. **Calculate Confidence and Variability**:\n",
    "   - After training, compute the **average confidence** and **variability** (standard deviation) for each data point.\n",
    "   - Use this data to generate a **data map**, which will help visualize how different examples fall into **easy-to-learn**, **hard-to-learn**, or **ambiguous** categories.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   data_map = []\n",
    "   for idx, confidences in confidence_records.items():\n",
    "       avg_confidence = np.mean(confidences)\n",
    "       variability = np.std(confidences)\n",
    "       data_map.append({'index': idx, 'avg_confidence': avg_confidence, 'variability': variability})\n",
    "   ```\n",
    "\n",
    "### **Step 3: Categorize Data Points**\n",
    "1. **Define Categories Based on Confidence and Variability**:\n",
    "   - **Easy-to-Learn**: High confidence (e.g., > **0.8**) and low variability (e.g., < **0.1**).\n",
    "   - **Hard-to-Learn**: Low confidence (e.g., < **0.4**) and low to moderate variability.\n",
    "   - **Ambiguous**: Moderate confidence (e.g., **0.4 - 0.6**) and high variability (e.g., > **0.3**).\n",
    "\n",
    "   ```python\n",
    "   easy_to_learn = [point for point in data_map if point['avg_confidence'] > 0.8 and point['variability'] < 0.1]\n",
    "   hard_to_learn = [point for point in data_map if point['avg_confidence'] < 0.4 and point['variability'] < 0.4]\n",
    "   ambiguous = [point for point in data_map if 0.4 <= point['avg_confidence'] <= 0.6 and point['variability'] > 0.3]\n",
    "   ```\n",
    "\n",
    "2. **Visualize the Data Map**:\n",
    "   - **Plot** the data points on a scatter plot with **variability** on the x-axis and **confidence** on the y-axis. This will help you visually inspect the distribution of examples.\n",
    "   \n",
    "   ```python\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   x_vals = [point['variability'] for point in data_map]\n",
    "   y_vals = [point['avg_confidence'] for point in data_map]\n",
    "\n",
    "   plt.scatter(x_vals, y_vals, color='blue', alpha=0.6)\n",
    "   plt.xlabel('Variability')\n",
    "   plt.ylabel('Confidence')\n",
    "   plt.title('Data Map: Confidence vs. Variability')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "### **Step 4: Retrain the Model on Specific Subsets**\n",
    "1. **Create a Retraining Dataset**:\n",
    "   - Use the data points categorized as **ambiguous** and **hard-to-learn** to create a retraining dataset.\n",
    "   - You could take, for example:\n",
    "     - **50% ambiguous examples**\n",
    "     - **25% hard-to-learn examples**\n",
    "     - **25% easy-to-learn examples**\n",
    "\n",
    "2. **Retrain from Checkpoint**:\n",
    "   - Load the model checkpoint from the last training session and **fine-tune** using the subset created. This helps the model improve on areas where it struggled before.\n",
    "   - Use a **lower learning rate** to ensure that the model refines its learning without drastically changing previously learned features.\n",
    "\n",
    "### **Step 5: Evaluate the Retrained Model**\n",
    "1. **Contrast Set Evaluation**:\n",
    "   - Evaluate the model using the **contrast set** to see if the model's performance has improved in handling nuanced or challenging examples.\n",
    "\n",
    "2. **Validation Accuracy**:\n",
    "   - Validate the model on the entire **SNLI validation set** to ensure that retraining did not lead to a decrease in general accuracy.\n",
    "\n",
    "### **Summary of Steps to Run Dataset Cartography on SNLI**\n",
    "1. **Prepare Data**: Load SNLI and choose a pre-trained model.\n",
    "2. **Training Loop with Tracking**: Track **confidence** and **variability** for each data point across epochs.\n",
    "3. **Calculate Metrics**: Compute **average confidence** and **variability** for each data point.\n",
    "4. **Categorize Examples**: Classify data points into **easy-to-learn**, **hard-to-learn**, and **ambiguous** based on the computed metrics.\n",
    "5. **Retrain the Model**: Use a subset focused on ambiguous and hard-to-learn examples to retrain the model.\n",
    "6. **Evaluate**: Evaluate the effectiveness of retraining using the contrast set and the SNLI validation set.\n",
    "\n",
    "Using Dataset Cartography in this manner allows you to gain a deeper understanding of how the model interacts with different parts of the dataset and helps in building a **more robust model** that generalizes better across diverse scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the **size of the retraining dataset**, you need to find an effective balance between providing sufficient diversity and complexity while not overwhelming the model with too much data to retrain. Given that the **contrast dataset** is **600 datapoints** and the **original SNLI validation set** is **10,000 datapoints**, an ideal retraining dataset should provide focused learning with a well-balanced subset of the original data.\n",
    "\n",
    "### **Proposed Size for Retraining Dataset**\n",
    "1. **Retraining Dataset Size: 1,500 to 3,000 Data Points**\n",
    "   - Considering the original dataset is **10,000 datapoints** and the contrast set is **600 datapoints**, a **retraining dataset of 1,500 to 3,000 datapoints** is generally effective. This size is sufficient to achieve focused improvements without excessively retraining on the entire dataset, allowing the model to refine its understanding in key areas.\n",
    "   - **1,500 datapoints** will provide a solid, concentrated retraining set that emphasizes the model's weaknesses, while **3,000 datapoints** will ensure more diverse retraining with broader coverage of easy-to-learn, hard-to-learn, and ambiguous examples.\n",
    "\n",
    "### **Why This Size?**\n",
    "- **Balance Between Targeted Learning and Generalization**:\n",
    "  - **1,500 to 3,000 examples** ensure that the retraining set is large enough to include a meaningful variety of examples without being as extensive as the original **10,000 datapoints**, which might dilute the focus on challenging or important examples.\n",
    "  - A smaller dataset, such as **600 examples**, might be too limited to reinforce learning across different types of data (easy, hard, ambiguous), whereas **1,500 to 3,000 examples** provide better coverage.\n",
    "\n",
    "- **Focusing on Ambiguous and Difficult Examples**:\n",
    "  - Ambiguous examples, in particular, require extra attention to improve model robustness. By using a moderately sized retraining set, you can ensure that ambiguous examples are sufficiently covered without overfitting or causing the model to forget previously learned relationships.\n",
    "  \n",
    "- **Efficient Training**:\n",
    "  - A retraining dataset of **1,500 to 3,000 examples** strikes a good balance between **efficiency** and **effectiveness**. This size is small enough to allow relatively quick retraining, yet it is large enough to cover the key areas that the model struggled with.\n",
    "\n",
    "### **Composition of the Retraining Dataset**\n",
    "Based on the proposed distribution mentioned earlier, you could create the **retraining dataset** using the following breakdown:\n",
    "\n",
    "#### For a **1,500-Example Retraining Dataset**:\n",
    "- **Ambiguous Examples**: **50%** (750 examples)\n",
    "- **Hard-to-Learn Examples**: **25%** (375 examples)\n",
    "- **Easy-to-Learn Examples**: **25%** (375 examples)\n",
    "\n",
    "#### For a **3,000-Example Retraining Dataset**:\n",
    "- **Ambiguous Examples**: **50%** (1,500 examples)\n",
    "- **Hard-to-Learn Examples**: **25%** (750 examples)\n",
    "- **Easy-to-Learn Examples**: **25%** (750 examples)\n",
    "\n",
    "### **How to Select the Data Points for Retraining**\n",
    "1. **Use Data Maps**: Utilize Dataset Cartography to map out and categorize the **easy-to-learn**, **hard-to-learn**, and **ambiguous examples** from the original **10,000 datapoints**.\n",
    "2. **Select Ambiguous Examples First**: Start by selecting **ambiguous examples** that fall in the range of **moderate confidence** and **high variability**. These should make up around **50%** of your retraining dataset.\n",
    "3. **Include Hard-to-Learn and Easy-to-Learn Examples**: \n",
    "   - Add **hard-to-learn examples** to provide the model with more exposure to difficult and complex relationships.\n",
    "   - Add **easy-to-learn examples** to maintain the foundation and avoid forgetting already learned representations.\n",
    "\n",
    "### **Training Strategy**\n",
    "- **Start from the Latest Checkpoint**: Start retraining from the most recent checkpoint rather than training from scratch. This helps retain previously learned features while focusing on specific areas needing improvement.\n",
    "- **Lower Learning Rate**: Use a **lower learning rate** to fine-tune the model on the selected data points. This prevents drastic weight updates that might lead to forgetting previous knowledge.\n",
    "- **Balanced Mini-Batches**: When retraining, each mini-batch should ideally contain a mix of **ambiguous**, **easy**, and **hard-to-learn** examples. This ensures that the model learns a balanced representation during each training step.\n",
    "\n",
    "### **Evaluation After Retraining**\n",
    "- **Contrast Set Evaluation**: After retraining, evaluate the model on the **contrast set** (600 examples) to verify if the retraining has indeed improved its handling of nuanced or complex examples.\n",
    "- **Validation on Full SNLI Set**: Also evaluate the model on the **entire SNLI validation set** to ensure that general accuracy has improved and that the model did not overfit during retraining.\n",
    "\n",
    "### **Summary of Approach**\n",
    "- Retraining Dataset Size: **1,500 to 3,000 examples**.\n",
    "- Composition: **50% Ambiguous**, **25% Hard-to-Learn**, **25% Easy-to-Learn**.\n",
    "- Select datapoints using **Dataset Cartography**.\n",
    "- Start from the **latest checkpoint**, use **lower learning rates**, and **balanced mini-batches**.\n",
    "- Evaluate on **contrast sets** and **full validation set** to confirm improvements in accuracy and robustness.\n",
    "\n",
    "This focused retraining strategy should help to improve the model's ability to handle difficult and ambiguous cases, resulting in better **accuracy** and more **robust generalization** to new and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of easy-to-learn points: 20392\n",
    "Number of hard-to-learn points: 1171\n",
    "Number of ambiguous points: 3638"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
